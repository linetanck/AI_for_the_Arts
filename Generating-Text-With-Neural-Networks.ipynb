{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c969bb8d",
   "metadata": {},
   "source": [
    "<b> `GUID:`</b> 2944377T  \n",
    "<b> `GitHub Repository:`https://github.com/linetanck/AI_for_the_Arts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e91816",
   "metadata": {},
   "source": [
    "# Generating text with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3df0a2",
   "metadata": {},
   "source": [
    "Welcome to this tutorial about generating text with the use of AI. Whether or not this is your first touch with AI or you are quite well read in the topic, you are at the right place. In the following, you will build, train and use an AI model to generate text in the style of William Shakespeare.  \n",
    "The AI method used are `neural networks`. Neural networks are a type of machine learning process that simmulate the human brain by using connected `nodes` and `layers` instead of neurons.  \n",
    "Firstly, we will start with preparing the data, then we will build and train the neural network. Lastly, the model can be used by giving the beginning of a Shakespeare quote and the AI will finish it.  \n",
    "  \n",
    "<b>Your question might be, why are we doing this?</b>  \n",
    "Of course it is fun to have a machine generate Shakespeare quotes, if you are well versed in literature it might even be funny to criticise the AI's writing. However, using text and AI has deeper implications. Inspecting the output of a text generation tells us a ot about our own language. Neural networks look at the language data we give it to train in a very different way than we do. A machine does not share all the underlying knowledge about the meaning of each words and their use together. For the neural network the data is abstract and the decisions it makes for the output are based on patters that were observed in the language data. These patterns are super interesting as they reveal things about language that we might not catch ourselves.<br>  <i>So, lets start discovering the capabilities of AI... </i>  \n",
    "![Shakespeare illustration, saying Lets go](images/Lets_go.png)  \n",
    "`Image source: own illustration`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70325e9",
   "metadata": {},
   "source": [
    "# 1. Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64f343",
   "metadata": {},
   "source": [
    "When working with AI, the data is very important, since it has an immense influence on the model. It is its substance. Bad data can mean that the model just does not perform well. Moreover, data that carries `bias` means the model carries bias. As an example, not recognizing racism in data leads to racism in an AI model's output, like in multiple AI controversies over the last few years.  \n",
    "Although a critical reflection of the data is crucial, it is not neccessary for this notebook and has been done beforehand. The need remains to inspect the data on a technical level though.  \n",
    "<b>`Code explanation:`</b>   \n",
    "In the following code, the `librairy` tensorflow is imported. Librairies are code written by other people that you can use for yourself. Tensorflow specifically is necessary to get the dataset and it provides the capabilities to build and train the model.  \n",
    "After importing tensorflow, the code gets the `dataset` from the shortcut URL and reads it.  \n",
    "The dataset consists of a Shakespeare text. Maybe copa and paste the link from the code and take a look, its always good to know your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url) # getting the file from the URL\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read() # reading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81aee2",
   "metadata": {},
   "source": [
    "It is always important to inspect the data because we need to know what it looks like after reading it in. Executing the following code `prints` an example snippet of how the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac39fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:80]) # not relevant to machine learning but relevant to exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4a899",
   "metadata": {},
   "source": [
    "Checking the data can already prevent errors happening. It also shows us if all the data was imported correctly. The shape of the data also influences the way it needs to be prepared and fed into the model, which we will be doing in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55b2aa",
   "metadata": {},
   "source": [
    "# 2. Preparing the Data\n",
    "There is quite a lot of transforming the data when working with AI.  \n",
    "For this project, the data is split into characters and then all letters are made lowercase. This is neccessary to later `encode` the data. Encoding is a crucial step when the AI is supposed to work with text. Since the computer handles the data as `binary data` (a collection of 0s and 1s), the text needs to be turned into binary data. This is happening during encoding.  \n",
    "<b>`Code explanation:`</b>  \n",
    "The tensorflow library is used to split the dataset into characters and placing everything in lowercase. Then, the data is being encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a12842",
   "metadata": {},
   "source": [
    "### Encoding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131f529",
   "metadata": {},
   "source": [
    "In the next step, the result of the encoding is printed out again to inspect it. Altough we might not be able to pull much information from the heap of numbers anymore, a few things are shown. The attribute `dtype` shows the data type of the encoded data. Since we need it to be numbers, we have to check if the data type is integers. Moreover, the attribute `shape` shows the size of the data. The second number after the comma is the amount of entries the data has, which will be important when `splitting` the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573e8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[21  7 10 ... 22 28 12]], shape=(1, 1115394), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(text_vec_layer([shakespeare_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7d7fd",
   "metadata": {},
   "source": [
    "### Tokenizing the Data\n",
    "The following code transforms the data by droping unwanted values and thereby preparing it for `tokenizing`. This means turning the data into units that are then processed by the AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd3ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690376b",
   "metadata": {},
   "source": [
    "Again, we inspect the result of transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c79b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1115394\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d971e",
   "metadata": {},
   "source": [
    "### Further Preparation\n",
    "In the following, the encoded and tokenized data is prepared further for the AI model. This is very important as the model will not run when the data does not fit what it expects.  \n",
    "Most importantly, the `batch size` is defined here. The batch size is the amount of entries of the data size the AI model takes one at a time during training. It cant take all of the data at once, so it needs to take a smaller portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8571a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32): # batch size is set to 32\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbfe03",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "The last step necessary for the data is splitting it. There are three processes that the model will run through: `training`, `validation`, and `testing`. For each process we need different data. The concept underlying is to check whether the model is only learning the training data off by heart or if it is actually able to do the same things on unseen new data. In one case, we could just be evaluating the model's knowledge about the data rather than the model's capabilities to generate text. Therefore, we split the data.  \n",
    "In the following code, the amount of data can be reduced to make the training time shorter. If your computing power is not that much (i.e.: working on a laptop) and you want to make training the model faster, the code indicates which numbers you have to change. <b>Its important to change all numbers as indicated or none!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42) # change the 1_000_000 to 100_000 to make it faster\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length) # change the 1_000_000 to 100_000 and the 1_060_000 to 106_000 to make it faster\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length) # change 1_060_000 to 112_000 to make it faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65051",
   "metadata": {},
   "source": [
    "# 3. Building and Training the Model  \n",
    "In the following step, the AI model is being built. With the type neural network, there are different layers being added. Understanding the layers in detail is very complicated and involves a lot of mathematics. A few important notes about the following code can be made though. Firstly, the first layer of a neural network is an activation layer that can have multiple types. For this model, it is a softmax layer which can broadly be said to transform the data into probabilities, which is neccessary as we need the words most likely to come after the input words as the output (See: Koech, Kiprono (2020): Softmax Activation Function — How It Actually Works. Medium. Avaliable Online at https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78 (Accessed 06.12.2023).)  \n",
    "The code also uses Adam as an `optimizer`. The optimizer is pert of what the model does without human command: the optimizer determines which nodes (the AI's neurons) are more important than others, it changes the `weights`. It independently adapts the neural network to perform better and better each run through. There are different kinds of optimizers but Adam is the staple one (See: Brownlee, Jason(2021): Gentle Introduction to the Adam Optimization Algorithm for Deep Learning. Machine Learning Mastery. Avaliable Online at https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ (Accessed 06.12.2023).)  \n",
    "During training, the model goes through `epochs`, which are specified in the code below to be 10. An epoch is the model going through the entire training dataset once. Overall, the model thus goes through the dataset 10 times, optimizing its setup each time to perform better and better.  \n",
    "You can watch the performance get better after each epoch by observing the `loss` and `accuracy` values change. These values are created in the following code as well. The accuracy describes how acccurate the predictions of the model are. The loss describes how far away from the desired output the model was.  \n",
    "\n",
    "<b>`Before executing:`</b>   \n",
    "Executing the following code will take a while. Depending on your processor it can take up to 12 hours. Make sure there are no other programs running in the background, this will make it even slower. It is also important that the computer does not set itself into standby or runs out of battery, as this will set back any progress. Dont restart the code either.  \n",
    "Grab a nice hot drink and sit back to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f19e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   3122/Unknown - 883s 279ms/step - loss: 1.7318 - accuracy: 0.4864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3122/3122 [==============================] - 900s 284ms/step - loss: 1.7318 - accuracy: 0.4864 - val_loss: 1.6767 - val_accuracy: 0.5103\n",
      "Epoch 2/10\n",
      "3122/3122 [==============================] - 1339s 425ms/step - loss: 1.1599 - accuracy: 0.6443 - val_loss: 1.9415 - val_accuracy: 0.4833\n",
      "Epoch 3/10\n",
      "3122/3122 [==============================] - 913s 289ms/step - loss: 1.0103 - accuracy: 0.6919 - val_loss: 2.0894 - val_accuracy: 0.4786\n",
      "Epoch 4/10\n",
      "3122/3122 [==============================] - 748s 236ms/step - loss: 0.9613 - accuracy: 0.7076 - val_loss: 2.1666 - val_accuracy: 0.4773\n",
      "Epoch 5/10\n",
      "3122/3122 [==============================] - 787s 249ms/step - loss: 0.9366 - accuracy: 0.7158 - val_loss: 2.2253 - val_accuracy: 0.4744\n",
      "Epoch 6/10\n",
      "3122/3122 [==============================] - 1169s 371ms/step - loss: 0.9210 - accuracy: 0.7210 - val_loss: 2.2585 - val_accuracy: 0.4749\n",
      "Epoch 7/10\n",
      "3122/3122 [==============================] - 2016s 641ms/step - loss: 0.9096 - accuracy: 0.7247 - val_loss: 2.2890 - val_accuracy: 0.4714\n",
      "Epoch 8/10\n",
      "3122/3122 [==============================] - 1173s 372ms/step - loss: 0.9007 - accuracy: 0.7277 - val_loss: 2.3169 - val_accuracy: 0.4723\n",
      "Epoch 9/10\n",
      "3122/3122 [==============================] - 909s 288ms/step - loss: 0.8932 - accuracy: 0.7301 - val_loss: 2.3191 - val_accuracy: 0.4712\n",
      "Epoch 10/10\n",
      "3122/3122 [==============================] - 1425s 453ms/step - loss: 0.8872 - accuracy: 0.7321 - val_loss: 2.3426 - val_accuracy: 0.4727\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\") # activation layer is set to be a softmax layer\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907eb89",
   "metadata": {},
   "source": [
    "In case you run into errors while training, make sure to read the `error message` carefully. A very good place to look up solutions is <a href=\"https://stackoverflow.com/\"> Stack Overflow </a>. Sometimes, error messages might pop up in red but can be ignored as they refer to potential future problems that we wont run into, since we wont work with the model furhter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84777190",
   "metadata": {},
   "source": [
    "### Reflecting the Training  \n",
    "Starting the training, in the first epoch the accuracy is at 0.4. This might be surprisingly low. Over all epochs, it does climb to 0.7 which is significantly better than the beginning but could be even higher. When using the model in the following steps, it is important to keep this in mind as it will reflect in the output. The fact that the accuracy is improving though, is a good sign. This means the model is learning!  \n",
    "Since this neural network is trained on a very small scale, this could be the reason for the improvable accuracy. Usually, when training a model, training takes place on a way bigger scale with more epochs and more data.  \n",
    "Nonetheless, the model is good enough to use in the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81beb95c",
   "metadata": {},
   "source": [
    "Before using the model to get some output, it has to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0216893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df202c4",
   "metadata": {},
   "source": [
    "## Interlude: Peer Discussion  \n",
    "The time it takes to run a model might be subjet to a peer discussion as it carries a number of implications.  \n",
    "For one, the time the model takes stands in relationship to the performance of the model in the sense that changing the code to make training faster often negatively impacts accuracy. There is a belance between the two that has to be found in machine learning. This balance needs to be reflected thoroughly and set very transparent to show how the project was amended to comply with the limited resources. <b> How much can we justify reducing training time in cost of accuracy? </b>  \n",
    "This shows the impact that the avaliable resources (time, money, computing power) have on the model's performance. These resources are often something that research projects in the Arts and Humanities do not have. Moreover, research projects might need financial sponsorship. This puts into question, whether such projects are still idependent research. <b> How independent should AI resarch be? </b>  \n",
    "With any resarch that is tied to finite resources, the question of accessibility of resarch and knowledge is raised. To prevent AI becoming inaccessible and tied to financial means, AI and machine learning knowledge needs to be spread to different disciplines, like in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927fee3",
   "metadata": {},
   "source": [
    "# 4. Generating Text\n",
    "Coming to the final part of the notebook, this is where it gets fun! In the following code, we can actually test out the performance of our very own AI model and see how good it knows Shakespeare.   \n",
    "<b>`Code explanation:`</b>  \n",
    "The fist line specifies our input in the model. We want the model to complete the sentence \"To be or not to b\" and the numbers specify, that we only one the last letter. Expecting an \"e\", lets see what the model puts out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0581742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 513ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af920647",
   "metadata": {},
   "source": [
    "That looks good! The next code it to calculate probabilities and draw 8 samples. Then we define a `function` called next_char that allows us to specify a parametre \"temperature\" that lets us manipulate the output more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b80ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fee9ad",
   "metadata": {},
   "source": [
    "In the next code, we define another function that extend the text to more characters than just the \"e\" like above. We do still need to include more code that allows reproducibility again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9085f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91583b00",
   "metadata": {},
   "source": [
    "Lets generate text longer than just a letter using the extend_text function. Will our model be able to extend the sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9632ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "To be or not to be promide,\n",
      "ere we ourry good madam, as a loves the \n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9326a7",
   "metadata": {},
   "source": [
    "In the next attempt, we increase the temperature to 1. How much will the output change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d1a893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 1s 548ms/step\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 498ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 319ms/step\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "1/1 [==============================] - 1s 599ms/step\n",
      "1/1 [==============================] - 0s 416ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 1s 947ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 1s 679ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 1s 728ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "To be or not to be provere inventy,\n",
      "begger your clead upon your voic\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc3706",
   "metadata": {},
   "source": [
    "Lets go extreme and see how much the output changes with a temperature of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c6803df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "To be or not to bef ,mt'&o3g:adm-$\n",
      "wh-nse?pws3ert--vgtsdjw!c-yje,znj\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1388c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
